---
title: How I got into the Hall of Fame
header-includes:
  - \usepackage{xcolor}
  - \usepackage{lipsum}
  - \usepackage{tabu}
  - \usepackage{booktabs}
  - \usepackage{colortbl}
  - \usepackage{float}
  - \setlength{\parskip}{0.25em}
  - \pagenumbering{arabic}
affiliation:
  ## use one only of the following
  # author-columnar: true         ## one column per author
  #institution-columnar: true  ## one column per institution (multiple autors eventually)
  wide: true                  ## one column wide author/affiliation fields

  institution:
    - name: Universitat Polit√®cnica de Catalunya
      department: Computer Science
      location: C/ Jordi Girona 1, Barcelona
      mark: 1
      author:
        - name: Unai Perez Mendizabal
          email: unai.perez.mendizabal@est.fib.upc.edu
        - name: Damian Rubio Cuervo
          email: damian.rubio.cuervo@est.fib.upc.edu
abstract: |
  The abstract goes here.
  On multiple lines eventually.

#bibliography: mybibfile.bib
output: rticles::ieee_article
---

```{r include=F}
library(DMwR)
library(chemometrics)
library(FactoMineR)
library(fpc)

baseball.dataset <- read.csv("../baseball.csv", na.strings = c('?'),
    colClasses = c("character", rep("integer", 11), rep("double", 4), rep("factor", 2)))
```

#Introduction

This paper aims to study the *Hall of Fame* dataset. It comprises a list of different american baseball players, including a collection of statistics about each one of them, corresponding to their whole careers. The Hall of Fame refers to an official recognition that a few organizations may (or may not) give to certain players with outstanding careers. The *Veterans Committee* (VC for short) and the *BaseBall Writers Association of America* (BBWAA) are two of these organizations. The statistics included in the dataset are the following:

- **`Player`**: Character. Variable representing the name of the player. It will not be used during the analysis, but only to label the individuals.
- **`Number_seasons`**: Count data representing how namy seasons the player has played in the league.
- **`Games_played`**: Count data representing how many games the player has played.
- **`At_bats`**: Count data representing the number of times a player has been in the hitter position.
- **`Runs`**: Count data representing the number of runs a player has achieved.
- **`Hits`**: Count data representing the number of hits a player has achieved.
- **`Doubles`**: Count data representing the number of doubles a player has achieved. This happens when the player runs through two bases after one hit.
- **`Triples`**: Count data representing the number of triples a player has achieved. This happens when the player runs through three bases after one hit.
- **`Home_runs`**: Count data representing the number of home runs a player has achieved.
- **`RBIs`**: Count data representing the number of runs-batted-in a player has achieved. That is the number of runs that were completed after one of the player's hit.
- **`Walks`**: Count data representing the number of walks a player has made. This happens when the pitcher throws the ball poorly four times and the batter is allowed to just walk to the first base.
- **`Strikeouts`**: Count data representing the number of times the player has been eliminated due to three strikes.
- **`Batting_average`**: Countinuous numerical data representing the average number of hits a player made when in the hitter position. 
- **`On_base_pct`**: Continuous data representing how frequently a batter reaches the base taking into account the times he has been in the batter position.
- **`Slugging_pct`**: Continuous data representing a measure of the batting productivity of a player. It is calculated with the formula below, where `1B` to `4B` refer to all possible outcomes of a hit (from single to home run) and `AB` refers to `At-Bats`. $$SLG = \frac{(1B) + (2 \times 2B) + (3 \times 3B) + (4 \times 4B)}{AB}$$
- **`Fielding_ave`**: Continuous data representing the average times a defensive player properly handles a batted or thrown ball.
- **`Position`**: Factor. Variable that represents the position of the player in the field. The possible values are catcher, designated hitter, first base, outfield (That encloses the positions of Right field, Left Field and Center Field), second base, short stop and third base. These are all defensive positions, except for the designated hitter.
- **`Hall_of_Fame`**: Categorical variable. This is the target variable. Originally, it has three categories that represent players not included in the hall of fame with 0, players included in the hall of fame by the Baseball Writers' Association of America (BBWAA) with 1, and players included in the hall of fame by the Veterans Committee (VC) with 2.

The dataset has `r nrow(baseball.dataset)` observations, from which, only `r length(which(baseball.dataset$Hall_of_Fame != 0))` are players that made it into the Hall of Fame.

# Data preprocessing

Values `1` and `2` of the class target `Hall_Of_Fame`, which is a factor, count as being in the Hall of Fame. The different values make a reference to the evaluation committee that accepted the players into the Hall. Depending on what procedure or analysis is run over the dataset, it might be useful to keep the different committees well labelled and differentiated. On the other hand, there are very few Hall of Fame players, so dividing them into two smaller categories seems unprofitable; the samples would get very small. The two categorizations have been kept for potential usage.

```{r include=FALSE, cache=F}
baseball.dataset.binary <- data.frame(baseball.dataset)
levels(baseball.dataset$Hall_of_Fame) <- c("no", "BBWAA", "VC")
levels(baseball.dataset.binary$Hall_of_Fame) <- c("no", "yes", "yes")
```

As for the individuals, the name `ELMER_SMITH` is duplicated. A quick search in Wikipedia tells that there are two former players by the name of Elmer Smith. The data matches: One had played 10 seasons and the other 14, so it is not mislabelled data, and removing it would mean data loss. According to Wikipedia, the one that played 14 seasons is however more generally known as Mike Smith. So its name was replaced to `MIKE_SMITH`, which does not match any other player in the set. Also, the players' names are originally considered as one variable. They are going to be used as the row names and the variable is going to be removed.

```{r include=FALSE, cache=F}
baseball.dataset[baseball.dataset$Player == "ELMER_SMITH" & baseball.dataset$Number_seasons == 14, "Player"] <- "MIKE_SMITH"

rownames(baseball.dataset) <- baseball.dataset$Player
baseball.dataset$Player <- NULL
```

## Missing data and imputation

Missing data in the dataset is represented as question marks (`?`). This happens to `r length(which(is.na(baseball.dataset$Strikeouts)))` observations in the `Strikeouts` variable. Only `r length(which(is.na(baseball.dataset$Strikeouts) & baseball.dataset$Hall_of_Fame != "no"))` out of the `r length(which(is.na(baseball.dataset$Strikeouts)))` is a Hall of Fame player. Actually, the hetereogeneity of the dataset, regarding the proportion of Hall of Fame and non Hall of Fame players, is pretty similar in the whole population and in the sample with missing values (`r round(nrow(baseball.dataset[baseball.dataset$Hall_of_Fame != "no",])/nrow(baseball.dataset), 3)` vs. `r nrow(baseball.dataset[baseball.dataset$Hall_of_Fame != "no" & is.na(baseball.dataset$Strikeouts),])/nrow(baseball.dataset[is.na(baseball.dataset$Strikeouts),])`). So, this sample has been considered relevant and has been imputed.

For this task, chained equations, PCA and kNN imputations have been tried. The chained equations method is not deterministic, but in most, if not all, of the attempts the imputed values decreased the mean of the variable significantly. PCA, on the contrary, increased the mean too much. kNN gave the results that most closely followed realistic values.

```{r include=F}
na.strikeouts <- which(is.na(baseball.dataset$Strikeouts))
baseball.dataset.knn <- knnImputation(baseball.dataset)
baseball.dataset.knn$Strikeouts <- round(baseball.dataset.knn$Strikeouts)
baseball.dataset$Strikeouts[na.strikeouts] <- baseball.dataset.knn$Strikeouts[na.strikeouts]
baseball.dataset.binary$Strikeouts[na.strikeouts] <- baseball.dataset.knn$Strikeouts[na.strikeouts]
```

## Outliers

Looking at the summary of the dataset, it seems like some variables have suspicious maximum and minimum values. For example, variable `RBIs` has a minimum value of 21. This value is far away from the mean of the variable and far below its first quartile. It can be seen in the boxplots of figure \ref{fig:rbi-univariate} that the range of the `RBIs` variable changes drastically when including or omitting the hall of fame players. The minimum value of `RBIs` for the hall of fame players is much higher than the overall minimum. This means that the dataset's discordant maximum and minimum values may not really be representative of univariate outliers, becuase of the two really different populations that the dataset mixes. This will also be applied to the rest of continuous variables that show suspicious values, such as `Walks` or `Strikeouts`.

```{r echo=FALSE, fig.height=3, fig.cap="\\label{fig:rbi-univariate}Univariate outlier detection over RBIs variable", cache=F}
par(mfrow=c(1, 3))
boxplot(baseball.dataset$RBIs, outcol="firebrick1", main="All players")
boxplot(baseball.dataset[baseball.dataset$Hall_of_Fame != "no", "RBIs"], outcol="firebrick1", main="Hall of fame")
boxplot(baseball.dataset[baseball.dataset$Hall_of_Fame == "no", "RBIs"], outcol="firebrick1", main="Not hall of fame")
par(mfrow=c(1, 1))
```

But, as for the averaging statistics, a different criteria should be applied. Specifically, the `Fielding_ave` variable has a maximum value of exactly `1`. Only one player, labelled as `r row.names(baseball.dataset)[which.max(baseball.dataset$Fielding_ave)]`, has the given value. This would mean that every time the player defended, the opposing team scored no runs. Still, the player did not make it into the Hall of Fame. Also, the mean of the variable is pretty high (`r mean(baseball.dataset$Fielding_ave)`). Without any more knowledge, this will not be considered an outlier.

As for multivariate outliers, the robustified Mahalanobis distance has been used for their detection. The plots of figure \ref{fig:rmahalanobis-distance} represent the distances of the individuals. The plot on the left shows in red those players that made it into the Hall of Fame, and in blue the ones that did not. It is obvious that the plot considers most of the players in the hall of fame as outliers, and there is a reason for that: They were considered for the hall of fame for their outstanding stats and performances indeed. These players kind of behave like outliers in real life, but that is the point. So, we have decided not to treat them as outliers.

```{r include=FALSE}
all.outliers <- Moutlier(baseball.dataset[,1:15], plot=F)
no.hall.outliers <- Moutlier(baseball.dataset[baseball.dataset$Hall_of_Fame == "no",1:15], plot=F)
par(mfrow=c(1,1))
```

```{r echo=FALSE, fig.cap="\\label{fig:rmahalanobis-distance}Mahalanobis distances with and without Hall of Fame", fig.pos="!t"}
par(mfrow=c(1,2))
plot(x=1:length(all.outliers$rd), y=all.outliers$rd, pch=19, cex=0.7, axes=F, xlab="", ylab="", col=as.character(lapply(baseball.dataset$Hall_of_Fame, function(x) ifelse(x == "no", "dodgerblue", "firebrick1"))))
abline(h=all.outliers$cutoff, lty=2, lwd=3)
axis(2, at=seq(from = 0, to = ceiling(max(all.outliers$rd)), by=(ceiling(max(all.outliers$rd))/5)), cex.axis=.7)
axis(1, cex.axis=.6)
no.hall.outlier.names <- rownames(baseball.dataset[baseball.dataset$Hall_of_Fame == "no",][order(no.hall.outliers$rd, decreasing = T)[1:round(nrow(baseball.dataset) * 0.025)],])
plot(no.hall.outliers$rd, pch=20, axes=F, col=as.character(lapply(rownames(baseball.dataset[baseball.dataset$Hall_of_Fame == "no",]), function(x) ifelse(x %in% no.hall.outlier.names, "firebrick1", "dodgerblue"))), xlab="", ylab="", cex=.7)
axis(2, at=seq(from = 0, to = ceiling(max(no.hall.outliers$rd)), by=ceiling(max(no.hall.outliers$rd)/5)), cex.axis=.7)
axis(1, cex.axis=.6)
par(mfrow=c(1,1))
```

The plot on the right of the above figure shows only players that are not in the hall of fame. It can be seen that some players still exceed other players. These could actually be considered outliers. If we consider to discard the 2.5% of the individuals, this is, `r nrow(baseball.dataset) * 0.025` with the greatest robustified Mahalanobis distance for being outliers, then those individuals plotted in red will be discarded. The player that most outlies from those excluded from the hall of fame actually has stats that are outstanding, so one can not help but wonder why he was not included. This player is Pete Rose and, as noted by Wikipedia, he has been involved in tax evasion and gambling controversy, which probably affected the commitees' decision making.

```{r include=FALSE, cache=F}
baseball.dataset <- baseball.dataset[-which(rownames(baseball.dataset) %in% no.hall.outlier.names),]
```

# Data exploration

Now we have to explore the data and obtain some basic graphs and conclusions: \lipsum[1]

# Clustering

```{r include=F}
baseball.dataset.pca <- data.frame(baseball.dataset)
baseball.dataset.pca[,1:15] <- scale(baseball.dataset.pca[,1:15])
pca.results <- PCA(baseball.dataset, quali.sup = 16:17)
```

```{r echo=FALSE, fig.cap="\\label{fig:pca-analysis}PCA Analysis of the Hall Of Fame.", fig.pos="!t"}
plot.PCA(pca.results, choix=c("varcor"))
```

In order to see what variables of the dataset characterize more the individuals we are going to perform some component analysis. This will also allow us to perform some clustering techniques with just the part of the information that is more relevant for our analysis. The analysis we are going to do is called PCA. When doing such an analysis we can see how the different variables are respresented and how much of the information is given by each of the analysis. In figure \ref{fig:pca-analysis} we can see how the variables of the dataset correlate with the relevant dimensions. As we can see, the variables best represented in the first dimension are `RBIs` and `Games_played`, while only `Fielding_ave` seems to correlate with the second dimension.

As we want to reduce the dimensionality of the data to be able to visualize and analyze it in a better way, we will have to decide how many dimensions are relvant to us. For that purpose we will use all that dimensions that will allow us to guarantee that we are preserving at least 80% of the information on the sample. In this case, that threshold is reached with just the three first dimensions. From now on, an in order to perform the clustering we will use the coordinates of the individuals in just those three dimensions.

```{r include=F}
baseball.data.matrix <- pca.results$ind$coord[,1:3]
baseball.distance.matrix <- dist(baseball.data.matrix, upper = TRUE, diag = TRUE)
baseball.hclust <- hclust(d = baseball.distance.matrix, method = 'ward.D2')
baseball.hclust.clusters <- cutree(baseball.hclust, k = 2)

# Comparing yes/no hall of fame results with the clusters
baseball.clustered <- cbind(baseball.dataset, baseball.hclust.clusters)
colnames(baseball.clustered) <- c(colnames(baseball.dataset), "Cluster")

# Players in cluster n1: 347
n1_total <- nrow(baseball.clustered[baseball.clustered$Cluster == 1,])
# Players in cluster n1 and Hall of fame: 116
n1_yes <- nrow(baseball.clustered[baseball.clustered$Cluster == 1 & baseball.clustered$Hall_of_Fame != 'no',])
# Players in cluster n2: 959
n2_total <- nrow(baseball.clustered[baseball.clustered$Cluster == 2,])
# Players in cluster n2 and Hall of fame: 9
n2_yes <- nrow(baseball.clustered[baseball.clustered$Cluster == 2 & baseball.clustered$Hall_of_Fame != 'no',])
```

The approach to clustering will be based on performing first a Hierarchical Clustering cutting it with the a priori knowledge we have, that is that there are only to clusters, one formed by players that entered the whole of fame and other with the ones that did not. In order to perform the clustering we will use the Ward distance metric. The results of this clustering are not trully representative of the classes that we know that exist in the data. If we cut the tree so that it gives us two clusters we will have `r n1_total` and `r n2_total` individuals in each cluster. The hall of fame players are divided as `r n1_yes` in the first cluster and `r n2_yes` in the second cluster. This tells us that `r round(n1_yes/(n1_yes+n2_yes)*100, 2)`% of them have been classified as been in the first cluster, but there they only represent `r round(n1_yes/n1_total*100, 2)`% of the total of individuals.

```{r include=F}
# Create a function to calculate the Calinski-Harabassz index
calinski_index = function(coordinates, hierarchical, n_clusters, consolidation) {
  # Get the clusters
  clusters = cutree(hierarchical, n_clusters)
  # Create a dataframe with the coordinates + the cluster
  df = data.frame(coordinates, Cluster = as.factor(clusters))
  if (consolidation == FALSE) { # if it is previous to consolidation step
    # Calculate the Calinski-Harabassz index
    index_value = calinhara(coordinates, df$Cluster, cn = max(clusters))
  }
  if (consolidation == TRUE) { # if it is posterior to consolidation step
    # Calculate the centroids with hierarchical
    centroids = aggregate(coordinates, list(df$Cluster), mean)
    # k-means taking as seeds the centroids calculated with hierarchical
    k_means = kmeans(coordinates, centers = centroids[,-1])
    # Create a dataframe with the coordinates + the k-means cluster
    df2 = data.frame(coordinates, Cluster = as.factor(k_means$cluster))
    # Calculate the Calinski-Harabassz index
    index_value = calinhara(coordinates, df2$Cluster, cn = max(k_means$cluster))
  }
  return(index_value)
}

# Create a vector with the Calinski index PRE consolidation for different values of k
baseball.calinski.hclust = calinski_index(pca.results$ind$coord[,1:3], baseball.hclust, 2, consolidation = FALSE)
for (i in 3:10){
  baseball.calinski.hclust = append(baseball.calinski.hclust, calinski_index(pca.results$ind$coord[,1:3], baseball.hclust, i, consolidation = FALSE))
}

# Create a vector with the Calinski index POST consolidation for different values of k
baseball.calinski.kmeans = calinski_index(pca.results$ind$coord[,1:3], baseball.hclust, 2, consolidation = TRUE)
for (i in 3:10){
  baseball.calinski.kmeans = append(baseball.calinski.kmeans, calinski_index(pca.results$ind$coord[,1:3], baseball.hclust, i, consolidation = TRUE))
}
```

```{r echo=FALSE, fig.cap="\\label{fig:calinski-harabassz}Clustering Evaluation by CH Index.", fig.pos="!t"}
# Plot the values of Calinski index
plot(baseball.calinski.kmeans, type = "o", xlab = 'Number of clusters', ylim=c(500,1300), ylab = 'Calinski value', main = 'Calinski-Harabassz Index', col = 'blue', xaxt = "n")
lines(baseball.calinski.hclust, type = "o", col = 'red', xaxt = "n")
axis(1, at=1:9, labels = c(2, 3, 4, 5, 6, 7, 8, 9, 10))
legend(5.5, 1320, legend=c("Index Before Consolodiation", "Index After Consolodiation"),
       col=c("red", "blue"), lty=c(1,1), cex=0.8)
```

In order to improve these results we are going to perform a consoloditaion step. This step will consist of a k-means clustering procedure that will start from the centroids of the clusters found in the hierarchical clustering and then iterationg until the optimal local solution is found. With the purpose of assesing to this step a metric that tells us if it improves the results we are going to use the Calinski-Harabassz index. This index has been run over both clusters, the hierarchical clustering without consolidation and the consolidated one. The results can be seen in figure \ref{fig:calinski-harabassz}. The greater the value of the index, the better the clustering has been. Thus, we should consider the consolidated clustering with just two clusters, the best result for this problem.

```{r include=F}
# Calculate the centroids with hierarchical
baseball.hclust.centroids = aggregate(baseball.data.matrix, list(baseball.hclust.clusters), mean)
baseball.cluster.consolidated = kmeans(baseball.data.matrix, centers = baseball.hclust.centroids[,-1])

# Comparing yes/no hall of fame results with the clusters
baseball.clustered <- cbind(baseball.dataset, baseball.cluster.consolidated$cluster)
colnames(baseball.clustered) <- c(colnames(baseball.dataset), "Cluster")

# Players in cluster n1: 347
n1_consolidated_total <- nrow(baseball.clustered[baseball.clustered$Cluster == 1,])
# Players in cluster n1 and Hall of fame: 116
n1_consolidated_yes <- nrow(baseball.clustered[baseball.clustered$Cluster == 1 & baseball.clustered$Hall_of_Fame != 'no',])
# Players in cluster n2: 959
n2_consolidated_total <- nrow(baseball.clustered[baseball.clustered$Cluster == 2,])
# Players in cluster n2 and Hall of fame: 9
n2_consolidated_yes <- nrow(baseball.clustered[baseball.clustered$Cluster == 2 & baseball.clustered$Hall_of_Fame != 'no',])
```

Now, the clustering provides us with the following results. The clusters split the baseball players so that cluster one contains `r n1_consolidated_total` and cluster two contains `r n2_consolidated_total` individuals. The hall of fame players are divided as `r n1_consolidated_yes` in the first cluster and `r n2_consolidated_yes` in the second cluster. This tells us that `r round(n1_consolidated_yes/(n1_consolidated_yes+n2_consolidated_yes)*100, 2)`% of the hall of fame players have been allocated into the first cluster, and that now they represent up to `r round(n1_consolidated_yes/n1_consolidated_total*100, 2)`% of the total of individuals in that cluster. These results prove to be much better than before. The distribution of the indivudals in the clusters over the first two dimensions found in the principal component analysis can be seen in figure \ref{fig:consolidated-clusters}.

```{r echo=FALSE, fig.cap="\\label{fig:consolidated-clusters}Clustering of Hall Of Fame.", fig.pos="!t"}
# Plot the clusters
plot(baseball.data.matrix[,1], baseball.data.matrix[,2],col=as.vector(baseball.clustered$Cluster), main = 'Consolidated Clustering', xlab = 'Dimension 1', ylab = 'Dimension 2')
legend("topleft", legend=c("Cluster 1", "Cluster 2"), pch=21, col=levels(as.factor(baseball.clustered$Cluster)))
```





<!-- An example of a floating figure using the graphicx package. -->
<!-- Note that \label must occur AFTER (or within) \caption. -->
<!-- For figures, \caption should occur after the \includegraphics. -->
<!-- Note that IEEEtran v1.7 and later has special internal code that -->
<!-- is designed to preserve the operation of \label within \caption -->
<!-- even when the captionsoff option is in effect. However, because -->
<!-- of issues like this, it may be the safest practice to put all your -->
<!-- \label just after \caption rather than within \caption{}. -->

<!-- Reminder: the "draftcls" or "draftclsnofoot", not "draft", class -->
<!-- option should be used if it is desired that the figures are to be -->
<!-- displayed while in draft mode. -->

<!-- \begin{figure}[!t] -->
<!-- \centering -->
<!-- \includegraphics[width=2.5in]{myfigure} -->
<!-- where an .eps filename suffix will be assumed under latex,  -->
<!-- and a .pdf suffix will be assumed for pdflatex; or what has been declared -->
<!-- via \DeclareGraphicsExtensions. -->
<!-- \caption{Simulation results for the network.} -->
<!-- \label{fig_sim} -->
<!-- \end{figure} -->

<!-- Note that the IEEE typically puts floats only at the top, even when this -->
<!-- results in a large percentage of a column being occupied by floats. -->


<!-- An example of a double column floating figure using two subfigures. -->
<!-- (The subfig.sty package must be loaded for this to work.) -->
<!-- The subfigure \label commands are set within each subfloat command, -->
<!-- and the \label for the overall figure must come after \caption. -->
<!-- \hfil is used as a separator to get equal spacing. -->
<!-- Watch out that the combined width of all the subfigures on a  -->
<!-- line do not exceed the text width or a line break will occur. -->

<!-- \begin{figure*}[!t] -->
<!-- \centering -->
<!-- \subfloat[Case I]{\includegraphics[width=2.5in]{box}% -->
<!-- \label{fig_first_case}} -->
<!-- \hfil -->
<!-- \subfloat[Case II]{\includegraphics[width=2.5in]{box}% -->
<!-- \label{fig_second_case}} -->
<!-- \caption{Simulation results for the network.} -->
<!-- \label{fig_sim} -->
<!-- \end{figure*} -->

<!-- Note that often IEEE papers with subfigures do not employ subfigure -->
<!-- captions (using the optional argument to \subfloat[]), but instead will -->
<!-- reference/describe all of them (a), (b), etc., within the main caption. -->
<!-- Be aware that for subfig.sty to generate the (a), (b), etc., subfigure -->
<!-- labels, the optional argument to \subfloat must be present. If a -->
<!-- subcaption is not desired, just leave its contents blank, -->
<!-- e.g., \subfloat[]. -->


<!-- An example of a floating table. Note that, for IEEE style tables, the -->
<!-- \caption command should come BEFORE the table and, given that table -->
<!-- captions serve much like titles, are usually capitalized except for words -->
<!-- such as a, an, and, as, at, but, by, for, in, nor, of, on, or, the, to -->
<!-- and up, which are usually not capitalized unless they are the first or -->
<!-- last word of the caption. Table text will default to \footnotesize as -->
<!-- the IEEE normally uses this smaller font for tables. -->
<!-- The \label must come after \caption as always. -->

<!-- \begin{table}[!t] -->
<!-- % increase table row spacing, adjust to taste -->
<!-- \renewcommand{\arraystretch}{1.3} -->
<!-- if using array.sty, it might be a good idea to tweak the value of -->
<!-- \extrarowheight as needed to properly center the text within the cells -->
<!-- \caption{An Example of a Table} -->
<!-- \label{table_example} -->
<!-- \centering -->
<!-- % Some packages, such as MDW tools, offer better commands for making tables -->
<!-- % than the plain LaTeX2e tabular which is used here. -->
<!-- \begin{tabular}{|c||c|} -->
<!-- \hline -->
<!-- One & Two\\ -->
<!-- \hline -->
<!-- Three & Four\\ -->
<!-- \hline -->
<!-- \end{tabular} -->
<!-- \end{table} -->


<!-- Note that the IEEE does not put floats in the very first column -->
<!-- - or typically anywhere on the first page for that matter. Also, -->
<!-- in-text middle ("here") positioning is typically not used, but it -->
<!-- is allowed and encouraged for Computer Society conferences (but -->
<!-- not Computer Society journals). Most IEEE journals/conferences use -->
<!-- top floats exclusively.  -->
<!-- Note that, LaTeX2e, unlike IEEE journals/conferences, places -->
<!-- footnotes above bottom floats. This can be corrected via the -->
<!-- \fnbelowfloat command of the stfloats package. -->


Conclusion
============
The conclusion goes here.

<!-- conference papers do not normally have an appendix -->

Acknowledgment {#acknowledgment}
==============

The authors would like to thank...

Bibliography styles
===================

<!-- Here are two sample references: @Feynman1963118 [@Dirac1953888]. -->

<!--\newpage-->
<!--References {#references .numbered}-->
